{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\bbala_n314ugx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bbala_n314ugx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bbala_n314ugx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bbala_n314ugx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bbala_n314ugx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from model import MultimodalModel, StockDataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tag based on % delta between close prices\n",
    "def label(curr_close, next_close):\n",
    "    pct_delta = (next_close - curr_close) / curr_close;\n",
    "    if pct_delta <= -0.01:\n",
    "        return 'bearish'\n",
    "    elif pct_delta > -0.01 and pct_delta < 0.01:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'bullish'\n",
    "df = pd.read_csv('data/stock_news_data_AAPL.csv', on_bad_lines='skip', low_memory=False)\n",
    "df = df.sort_values(by=['date'])\n",
    "df.set_index('date',inplace=True)\n",
    "\n",
    "# labels dataset\n",
    "labels = []\n",
    "for i in range(len(df)-1):\n",
    "    curr_close = df.iloc[i]['Stock Close']\n",
    "    next_close = df.iloc[i+1]['Stock Close']\n",
    "    l = label(curr_close, next_close)\n",
    "    labels.append(l)\n",
    "labels.append(None)\n",
    "df['label'] = labels\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article URL</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>overall_sentiment_score</th>\n",
       "      <th>overall_sentiment_label</th>\n",
       "      <th>Stock Open</th>\n",
       "      <th>Stock Close</th>\n",
       "      <th>Stock High</th>\n",
       "      <th>Stock Low</th>\n",
       "      <th>volume</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>US stocks fall, oil tops $105 as Ukraine crisi...</td>\n",
       "      <td>https://www.aljazeera.com/economy/2022/3/1/us-...</td>\n",
       "      <td>A surge in oil sent shivers through risky asse...</td>\n",
       "      <td>-0.277460</td>\n",
       "      <td>Somewhat-Bearish</td>\n",
       "      <td>164.695</td>\n",
       "      <td>163.20</td>\n",
       "      <td>166.60</td>\n",
       "      <td>161.97</td>\n",
       "      <td>79455454.0</td>\n",
       "      <td>701957.0</td>\n",
       "      <td>164.167482</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>Rich Russians turn to luxury jewellery, watche...</td>\n",
       "      <td>https://www.aljazeera.com/economy/2022/3/2/ric...</td>\n",
       "      <td>With sanctions on Russia sending the ruble plu...</td>\n",
       "      <td>-0.118323</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>164.390</td>\n",
       "      <td>166.56</td>\n",
       "      <td>167.36</td>\n",
       "      <td>162.95</td>\n",
       "      <td>76135254.0</td>\n",
       "      <td>631927.0</td>\n",
       "      <td>165.810466</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-03</th>\n",
       "      <td>Are You an Investor Needing Some Calm Guidance?</td>\n",
       "      <td>https://www.fool.com/investing/2022/03/03/are-...</td>\n",
       "      <td>Read this.</td>\n",
       "      <td>-0.041040</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>168.470</td>\n",
       "      <td>166.23</td>\n",
       "      <td>168.91</td>\n",
       "      <td>165.55</td>\n",
       "      <td>73779442.0</td>\n",
       "      <td>622341.0</td>\n",
       "      <td>166.927454</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-04</th>\n",
       "      <td>Marvell's  ( MRVL )  Q4 Earnings and Revenues ...</td>\n",
       "      <td>https://www.zacks.com/stock/news/1877623/marve...</td>\n",
       "      <td>Marvell's (MRVL) Q4 top and bottom lines refle...</td>\n",
       "      <td>0.136708</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>164.490</td>\n",
       "      <td>163.17</td>\n",
       "      <td>165.55</td>\n",
       "      <td>162.10</td>\n",
       "      <td>80761684.0</td>\n",
       "      <td>710586.0</td>\n",
       "      <td>163.402599</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-07</th>\n",
       "      <td>EPAM Shares Continue to Fall on Ukraine Crisis...</td>\n",
       "      <td>https://www.zacks.com/stock/news/1878525/epam-...</td>\n",
       "      <td>EPAM Systems' (EPAM) share price has plunged s...</td>\n",
       "      <td>-0.046687</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>163.360</td>\n",
       "      <td>159.30</td>\n",
       "      <td>165.02</td>\n",
       "      <td>159.04</td>\n",
       "      <td>92893526.0</td>\n",
       "      <td>803961.0</td>\n",
       "      <td>161.403790</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article Headline  \\\n",
       "date                                                            \n",
       "2022-03-01  US stocks fall, oil tops $105 as Ukraine crisi...   \n",
       "2022-03-02  Rich Russians turn to luxury jewellery, watche...   \n",
       "2022-03-03    Are You an Investor Needing Some Calm Guidance?   \n",
       "2022-03-04  Marvell's  ( MRVL )  Q4 Earnings and Revenues ...   \n",
       "2022-03-07  EPAM Shares Continue to Fall on Ukraine Crisis...   \n",
       "\n",
       "                                                  Article URL  \\\n",
       "date                                                            \n",
       "2022-03-01  https://www.aljazeera.com/economy/2022/3/1/us-...   \n",
       "2022-03-02  https://www.aljazeera.com/economy/2022/3/2/ric...   \n",
       "2022-03-03  https://www.fool.com/investing/2022/03/03/are-...   \n",
       "2022-03-04  https://www.zacks.com/stock/news/1877623/marve...   \n",
       "2022-03-07  https://www.zacks.com/stock/news/1878525/epam-...   \n",
       "\n",
       "                                                 Article Text  \\\n",
       "date                                                            \n",
       "2022-03-01  A surge in oil sent shivers through risky asse...   \n",
       "2022-03-02  With sanctions on Russia sending the ruble plu...   \n",
       "2022-03-03                                         Read this.   \n",
       "2022-03-04  Marvell's (MRVL) Q4 top and bottom lines refle...   \n",
       "2022-03-07  EPAM Systems' (EPAM) share price has plunged s...   \n",
       "\n",
       "            overall_sentiment_score overall_sentiment_label  Stock Open  \\\n",
       "date                                                                      \n",
       "2022-03-01                -0.277460        Somewhat-Bearish     164.695   \n",
       "2022-03-02                -0.118323                 Neutral     164.390   \n",
       "2022-03-03                -0.041040                 Neutral     168.470   \n",
       "2022-03-04                 0.136708                 Neutral     164.490   \n",
       "2022-03-07                -0.046687                 Neutral     163.360   \n",
       "\n",
       "            Stock Close  Stock High  Stock Low      volume  num_trades  \\\n",
       "date                                                                     \n",
       "2022-03-01       163.20      166.60     161.97  79455454.0    701957.0   \n",
       "2022-03-02       166.56      167.36     162.95  76135254.0    631927.0   \n",
       "2022-03-03       166.23      168.91     165.55  73779442.0    622341.0   \n",
       "2022-03-04       163.17      165.55     162.10  80761684.0    710586.0   \n",
       "2022-03-07       159.30      165.02     159.04  92893526.0    803961.0   \n",
       "\n",
       "             adj_close    label  \n",
       "date                             \n",
       "2022-03-01  164.167482  bullish  \n",
       "2022-03-02  165.810466  neutral  \n",
       "2022-03-03  166.927454  bearish  \n",
       "2022-03-04  163.402599  bearish  \n",
       "2022-03-07  161.403790  bearish  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "dataset = StockDataset(df, extraction_type='bert', lookback=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article URL</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>overall_sentiment_score</th>\n",
       "      <th>overall_sentiment_label</th>\n",
       "      <th>Stock Open</th>\n",
       "      <th>Stock Close</th>\n",
       "      <th>Stock High</th>\n",
       "      <th>Stock Low</th>\n",
       "      <th>volume</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>label</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>ATR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXCLUSIVE from Bitcoin 2022: TradeZing CEO Jor...</td>\n",
       "      <td>https://www.benzinga.com/markets/cryptocurrenc...</td>\n",
       "      <td>detroit-based benzinga , a medium and data pro...</td>\n",
       "      <td>-0.055666</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>163.92</td>\n",
       "      <td>165.07</td>\n",
       "      <td>166.5984</td>\n",
       "      <td>163.57</td>\n",
       "      <td>68843424.0</td>\n",
       "      <td>574580.0</td>\n",
       "      <td>164.928106</td>\n",
       "      <td>bullish</td>\n",
       "      <td>166.920000</td>\n",
       "      <td>166.177407</td>\n",
       "      <td>27.659200</td>\n",
       "      <td>2.103217</td>\n",
       "      <td>4.206650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China Lockdowns, Factory Closures Seen Hurting...</td>\n",
       "      <td>https://www.investors.com/news/technology/appl...</td>\n",
       "      <td>apple stock : china lockdown , factory closure...</td>\n",
       "      <td>-0.051895</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>165.02</td>\n",
       "      <td>167.40</td>\n",
       "      <td>167.8200</td>\n",
       "      <td>163.91</td>\n",
       "      <td>67566065.0</td>\n",
       "      <td>546660.0</td>\n",
       "      <td>166.548430</td>\n",
       "      <td>neutral</td>\n",
       "      <td>165.920000</td>\n",
       "      <td>166.788704</td>\n",
       "      <td>51.523573</td>\n",
       "      <td>1.789685</td>\n",
       "      <td>4.107767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXCLUSIVE From Bitcoin 2022: How To Increase L...</td>\n",
       "      <td>https://www.benzinga.com/markets/cryptocurrenc...</td>\n",
       "      <td>detroit-based benzinga , a medium and data pro...</td>\n",
       "      <td>-0.053154</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>168.76</td>\n",
       "      <td>167.23</td>\n",
       "      <td>168.8800</td>\n",
       "      <td>166.10</td>\n",
       "      <td>67515353.0</td>\n",
       "      <td>590666.0</td>\n",
       "      <td>167.349497</td>\n",
       "      <td>neutral</td>\n",
       "      <td>166.566667</td>\n",
       "      <td>167.009352</td>\n",
       "      <td>49.728204</td>\n",
       "      <td>1.510084</td>\n",
       "      <td>3.665178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garmin  ( GRMN )  Strengthens Fitness Segment ...</td>\n",
       "      <td>https://www.zacks.com/stock/news/1904502/garmi...</td>\n",
       "      <td>garmin ( grmn ) expand fitness offering with t...</td>\n",
       "      <td>0.151760</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>168.91</td>\n",
       "      <td>166.42</td>\n",
       "      <td>171.5300</td>\n",
       "      <td>165.91</td>\n",
       "      <td>86895494.0</td>\n",
       "      <td>748146.0</td>\n",
       "      <td>168.808299</td>\n",
       "      <td>bearish</td>\n",
       "      <td>167.016667</td>\n",
       "      <td>166.714676</td>\n",
       "      <td>39.813029</td>\n",
       "      <td>1.209199</td>\n",
       "      <td>4.316785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNAP Q1 Earnings Miss Estimates, User Growth A...</td>\n",
       "      <td>https://www.zacks.com/stock/news/1905492/snap-...</td>\n",
       "      <td>snap 's ( snap ) first-quarter result reflect ...</td>\n",
       "      <td>-0.051587</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>166.46</td>\n",
       "      <td>161.79</td>\n",
       "      <td>167.8699</td>\n",
       "      <td>161.50</td>\n",
       "      <td>84550744.0</td>\n",
       "      <td>725782.0</td>\n",
       "      <td>164.231066</td>\n",
       "      <td>neutral</td>\n",
       "      <td>165.146667</td>\n",
       "      <td>164.252338</td>\n",
       "      <td>14.693530</td>\n",
       "      <td>0.590338</td>\n",
       "      <td>5.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Democratic presidential longshot Marianne Will...</td>\n",
       "      <td>https://www.marketwatch.com/story/democratic-p...</td>\n",
       "      <td>democrat be close rank behind president biden ...</td>\n",
       "      <td>0.107952</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>164.59</td>\n",
       "      <td>165.21</td>\n",
       "      <td>166.3200</td>\n",
       "      <td>163.83</td>\n",
       "      <td>48280881.0</td>\n",
       "      <td>476387.0</td>\n",
       "      <td>165.025855</td>\n",
       "      <td>neutral</td>\n",
       "      <td>163.623333</td>\n",
       "      <td>164.262721</td>\n",
       "      <td>69.202059</td>\n",
       "      <td>2.973583</td>\n",
       "      <td>3.293847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Apple Offers High-Yield Savings Accounts in La...</td>\n",
       "      <td>https://www.barrons.com/articles/apple-card-sa...</td>\n",
       "      <td>apple card saving account be the company 's la...</td>\n",
       "      <td>0.114923</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>165.09</td>\n",
       "      <td>165.23</td>\n",
       "      <td>165.3900</td>\n",
       "      <td>164.03</td>\n",
       "      <td>41531918.0</td>\n",
       "      <td>484303.0</td>\n",
       "      <td>164.804537</td>\n",
       "      <td>neutral</td>\n",
       "      <td>165.333333</td>\n",
       "      <td>164.746361</td>\n",
       "      <td>69.363014</td>\n",
       "      <td>2.963389</td>\n",
       "      <td>2.649232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Apple Card Savings Account Launched! Check min...</td>\n",
       "      <td>https://www.financialexpress.com/business/inve...</td>\n",
       "      <td>apple card saving account launch ! check minim...</td>\n",
       "      <td>0.254145</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>166.10</td>\n",
       "      <td>166.47</td>\n",
       "      <td>167.4100</td>\n",
       "      <td>165.65</td>\n",
       "      <td>49948656.0</td>\n",
       "      <td>495451.0</td>\n",
       "      <td>166.353505</td>\n",
       "      <td>neutral</td>\n",
       "      <td>165.636667</td>\n",
       "      <td>165.608180</td>\n",
       "      <td>79.383367</td>\n",
       "      <td>3.020549</td>\n",
       "      <td>2.492821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Committed to investing across the country: App...</td>\n",
       "      <td>https://www.business-standard.com/india-news/c...</td>\n",
       "      <td>commit to invest across the country : apple ce...</td>\n",
       "      <td>0.424460</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>165.80</td>\n",
       "      <td>167.63</td>\n",
       "      <td>168.1600</td>\n",
       "      <td>165.54</td>\n",
       "      <td>47848162.0</td>\n",
       "      <td>475096.0</td>\n",
       "      <td>167.278824</td>\n",
       "      <td>neutral</td>\n",
       "      <td>166.443333</td>\n",
       "      <td>166.619090</td>\n",
       "      <td>85.868847</td>\n",
       "      <td>3.123446</td>\n",
       "      <td>2.535214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Why Taiwan Semiconductor Manufacturing Stock R...</td>\n",
       "      <td>https://www.fool.com/investing/2023/04/20/why-...</td>\n",
       "      <td>the semiconductor specialist 's result hold up...</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>166.09</td>\n",
       "      <td>166.65</td>\n",
       "      <td>167.8700</td>\n",
       "      <td>165.56</td>\n",
       "      <td>52455343.0</td>\n",
       "      <td>493146.0</td>\n",
       "      <td>166.779014</td>\n",
       "      <td>neutral</td>\n",
       "      <td>166.916667</td>\n",
       "      <td>166.634545</td>\n",
       "      <td>61.394415</td>\n",
       "      <td>3.090291</td>\n",
       "      <td>2.460143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Article Headline  \\\n",
       "0    EXCLUSIVE from Bitcoin 2022: TradeZing CEO Jor...   \n",
       "1    China Lockdowns, Factory Closures Seen Hurting...   \n",
       "2    EXCLUSIVE From Bitcoin 2022: How To Increase L...   \n",
       "3    Garmin  ( GRMN )  Strengthens Fitness Segment ...   \n",
       "4    SNAP Q1 Earnings Miss Estimates, User Growth A...   \n",
       "..                                                 ...   \n",
       "249  Democratic presidential longshot Marianne Will...   \n",
       "250  Apple Offers High-Yield Savings Accounts in La...   \n",
       "251  Apple Card Savings Account Launched! Check min...   \n",
       "252  Committed to investing across the country: App...   \n",
       "253  Why Taiwan Semiconductor Manufacturing Stock R...   \n",
       "\n",
       "                                           Article URL  \\\n",
       "0    https://www.benzinga.com/markets/cryptocurrenc...   \n",
       "1    https://www.investors.com/news/technology/appl...   \n",
       "2    https://www.benzinga.com/markets/cryptocurrenc...   \n",
       "3    https://www.zacks.com/stock/news/1904502/garmi...   \n",
       "4    https://www.zacks.com/stock/news/1905492/snap-...   \n",
       "..                                                 ...   \n",
       "249  https://www.marketwatch.com/story/democratic-p...   \n",
       "250  https://www.barrons.com/articles/apple-card-sa...   \n",
       "251  https://www.financialexpress.com/business/inve...   \n",
       "252  https://www.business-standard.com/india-news/c...   \n",
       "253  https://www.fool.com/investing/2023/04/20/why-...   \n",
       "\n",
       "                                          Article Text  \\\n",
       "0    detroit-based benzinga , a medium and data pro...   \n",
       "1    apple stock : china lockdown , factory closure...   \n",
       "2    detroit-based benzinga , a medium and data pro...   \n",
       "3    garmin ( grmn ) expand fitness offering with t...   \n",
       "4    snap 's ( snap ) first-quarter result reflect ...   \n",
       "..                                                 ...   \n",
       "249  democrat be close rank behind president biden ...   \n",
       "250  apple card saving account be the company 's la...   \n",
       "251  apple card saving account launch ! check minim...   \n",
       "252  commit to invest across the country : apple ce...   \n",
       "253  the semiconductor specialist 's result hold up...   \n",
       "\n",
       "     overall_sentiment_score overall_sentiment_label  Stock Open  Stock Close  \\\n",
       "0                  -0.055666                 Neutral      163.92       165.07   \n",
       "1                  -0.051895                 Neutral      165.02       167.40   \n",
       "2                  -0.053154                 Neutral      168.76       167.23   \n",
       "3                   0.151760        Somewhat-Bullish      168.91       166.42   \n",
       "4                  -0.051587                 Neutral      166.46       161.79   \n",
       "..                       ...                     ...         ...          ...   \n",
       "249                 0.107952                 Neutral      164.59       165.21   \n",
       "250                 0.114923                 Neutral      165.09       165.23   \n",
       "251                 0.254145        Somewhat-Bullish      166.10       166.47   \n",
       "252                 0.424460                 Bullish      165.80       167.63   \n",
       "253                 0.096941                 Neutral      166.09       166.65   \n",
       "\n",
       "     Stock High  Stock Low      volume  num_trades   adj_close    label  \\\n",
       "0      166.5984     163.57  68843424.0    574580.0  164.928106  bullish   \n",
       "1      167.8200     163.91  67566065.0    546660.0  166.548430  neutral   \n",
       "2      168.8800     166.10  67515353.0    590666.0  167.349497  neutral   \n",
       "3      171.5300     165.91  86895494.0    748146.0  168.808299  bearish   \n",
       "4      167.8699     161.50  84550744.0    725782.0  164.231066  neutral   \n",
       "..          ...        ...         ...         ...         ...      ...   \n",
       "249    166.3200     163.83  48280881.0    476387.0  165.025855  neutral   \n",
       "250    165.3900     164.03  41531918.0    484303.0  164.804537  neutral   \n",
       "251    167.4100     165.65  49948656.0    495451.0  166.353505  neutral   \n",
       "252    168.1600     165.54  47848162.0    475096.0  167.278824  neutral   \n",
       "253    167.8700     165.56  52455343.0    493146.0  166.779014  neutral   \n",
       "\n",
       "            SMA         EMA        RSI      MACD       ATR  \n",
       "0    166.920000  166.177407  27.659200  2.103217  4.206650  \n",
       "1    165.920000  166.788704  51.523573  1.789685  4.107767  \n",
       "2    166.566667  167.009352  49.728204  1.510084  3.665178  \n",
       "3    167.016667  166.714676  39.813029  1.209199  4.316785  \n",
       "4    165.146667  164.252338  14.693530  0.590338  5.001157  \n",
       "..          ...         ...        ...       ...       ...  \n",
       "249  163.623333  164.262721  69.202059  2.973583  3.293847  \n",
       "250  165.333333  164.746361  69.363014  2.963389  2.649232  \n",
       "251  165.636667  165.608180  79.383367  3.020549  2.492821  \n",
       "252  166.443333  166.619090  85.868847  3.123446  2.535214  \n",
       "253  166.916667  166.634545  61.394415  3.090291  2.460143  \n",
       "\n",
       "[254 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# TODO: padding/truncate here\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataset, val_dataset = Subset(dataset, range(train_size)), Subset(dataset, range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) == train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)==val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = MultimodalModel(hidden_size=64, num_classes=5, extraction_type='bert', lstm_or_gru='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# for target in train_loader:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# print(type(target), target)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# break\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m article_input, numerical_data, target \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     10\u001b[0m     \u001b[39m# article_input = {k: v.to(device) for k, v in article_input.items()}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# numerical_data = numerical_data.to(device)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[39m=\u001b[39m model(article_input, numerical_data)\n\u001b[0;32m     16\u001b[0m     \u001b[39m# Compute loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\CS\\USC\\USC_CSCI544_Spring23\\CSCI544-FinContext\\model.py:76\u001b[0m, in \u001b[0;36mMultimodalModel.forward\u001b[1;34m(self, article_input, numerical_data)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextraction_type \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfinbert\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     73\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m     74\u001b[0m         article_input, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[1;32m---> 76\u001b[0m     text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     77\u001b[0m     text_embed \u001b[39m=\u001b[39m text_outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    496\u001b[0m         hidden_states,\n\u001b[0;32m    497\u001b[0m         attention_mask,\n\u001b[0;32m    498\u001b[0m         head_mask,\n\u001b[0;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:434\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[1;32m--> 434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:384\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 384\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    385\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    386\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bbala_n314ugx\\mambaforge\\envs\\csci544_2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from sklearn.metrics import classification_report\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # for target in train_loader:\n",
    "        # print(type(target), target)\n",
    "        # break\n",
    "    for article_input, numerical_data, target in train_loader:\n",
    "        # article_input = {k: v.to(device) for k, v in article_input.items()}\n",
    "        # numerical_data = numerical_data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(article_input, numerical_data)\n",
    "        \n",
    "        # Compute loss\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        # targets = target.to(device)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        allpred, alltarg = [],[]\n",
    "        for article_input, numerical_data, target in val_loader:\n",
    "            # article_input = {k: v.to(device) for k, v in article_input.items()}\n",
    "            # numerical_data = numerical_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(article_input, numerical_data)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            _, targets = torch.max(outputs, dim=1)\n",
    "            allpred.extend(predicted.cpu().numpy())            \n",
    "            alltarg.append(targets.cpu().numpy())\n",
    "            total_correct += (predicted == target).sum().item()\n",
    "    \n",
    "    print(classification_report(allpred, alltarg))\n",
    "    accuracy = total_correct / val_size\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
